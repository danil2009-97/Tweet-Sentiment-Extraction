{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final_function","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2JAlnmlOiBXX"},"source":["**Function1**\n","\n","  - Take raw data as input and return Predictions for that point"]},{"cell_type":"code","metadata":{"id":"N_oXaUgIl3o4","executionInfo":{"status":"ok","timestamp":1603683786917,"user_tz":-330,"elapsed":3625,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}},"outputId":"9b160b88-28d5-483a-eabc-5e3bd480d6a5","colab":{"base_uri":"https://localhost:8080/","height":390}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tio_fpqxl4HW","executionInfo":{"status":"ok","timestamp":1603683786918,"user_tz":-330,"elapsed":3615,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}},"outputId":"1c55e239-b420-40c8-e4bb-799c95c90c55","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mBfaF3FWpo_-","executionInfo":{"status":"ok","timestamp":1603683788672,"user_tz":-330,"elapsed":5362,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGDEBAdOliFk","executionInfo":{"status":"ok","timestamp":1603683788673,"user_tz":-330,"elapsed":5357,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":["def get_predictions(input):\n","  \"\"\"This function returns the predictions for the given input\"\"\"\n","  from transformers import TFRobertaForQuestionAnswering, TFRobertaModel\n","  roberta = TFRobertaForQuestionAnswering.from_pretrained('/content/drive/My Drive/tweet-sentiment-extraction/mymodel_pretrained')\n","  print('*'*50)\n","  print('Loaded Pretrained TFRobertaForQuestionAnswering model')\n","  print('*'*50)\n","  MAX_LEN=128\n","  import tensorflow as tf\n","  from tensorflow.keras.models import Model\n","  from tensorflow.keras.layers import Input,Softmax,Dense,Activation,Dropout,Flatten\n","\n","\n","  input1 = Input(shape=(MAX_LEN,),name='input_id',dtype=tf.int32)\n","  input2 = Input(shape=(MAX_LEN,),name='attention_mask',dtype=tf.int32)\n","  start_scores,end_scores = roberta(input1,attention_mask = input2)\n","  drop1 = Dropout(0.1)(start_scores)\n","  drop1  = tf.expand_dims(drop1,axis=-1)\n","  layer1 = tf.keras.layers.Conv1D(1,1)(drop1)\n","  layer1= Flatten()(layer1)\n","  softmax1 = Activation('softmax')(layer1)\n","  \n","  drop2 = Dropout(0.1)(end_scores)\n","  drop2  = tf.expand_dims(drop2,axis=-1)\n","  layer2 = tf.keras.layers.Conv1D(1,1)(drop2)\n","  layer2 = Flatten()(layer2)\n","  softmax2 = Activation('softmax')(layer2)\n","\n","  model = Model(inputs=[input1,input2],outputs=[softmax1,softmax2])\n","  model.load_weights('/content/drive/My Drive/tweet-sentiment-extraction/checkpt1/model_roberta.hdf5')\n","  print('Loaded trained model')\n","  print('*'*50)\n","\n","  print('Preprocessing input data')\n","  print('*'*50)\n","  input['text'] = input['text'].apply(lambda x : str(x).lower())\n","\n","  count = input.shape[0]\n","  input_ids = np.zeros((count,MAX_LEN),dtype='int32')\n","  attention_mask = np.zeros((count,MAX_LEN),dtype='int32')\n","\n","  ip_data = input[['text','sentiment']]\n","  print('Loading Pretrained Tokenizer for TfRoberta model')\n","  print('*'*50)\n","  from transformers import RobertaTokenizer\n","  tokenizer = RobertaTokenizer.from_pretrained('/content/drive/My Drive/tweet-sentiment-extraction/roberta_tokenizer',add_prefix_space=True)\n","\n","  print('Getting input_ids and attention_mask for the input')\n","  print('*'*50)\n","  from tqdm import tqdm\n","  for i,each in tqdm(enumerate(ip_data.values)):\n","\n","    val = tokenizer.encode_plus(each[0],each[1],add_special_tokens=True,max_length=128,return_attention_mask=True,pad_to_max_length=True,return_tensors='tf',verbose=False)\n","    input_ids[i] = val['input_ids']\n","    attention_mask[i] = val['attention_mask']\n","\n","  print('Shape of input id and attention mask:',input_ids.shape,attention_mask.shape)\n","  print('*'*50)\n","  print('Predicting o/p value')\n","  print('*'*50)\n","  input_data = (input_ids,attention_mask)\n","  start_pred , end_pred = model.predict((input_data))\n","\n","\n","  pred_text=[]\n","  for i in range(input_ids.shape[0]):\n","    a = np.argmax(start_pred[i])\n","    b=np.argmax(end_pred[i])\n","    text1 = \" \"+\" \".join(ip_data['text'].values[i].split())\n","    enc = tokenizer.encode(text1)\n","    st = tokenizer.decode(enc[a:b+1])\n","    pred_text.append(st)\n","\n","  print('Successfully ran!!!!!')\n","  print('*'*50)\n","  return pred_text"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDEjKsZqliBp","executionInfo":{"status":"ok","timestamp":1603683788674,"user_tz":-330,"elapsed":5350,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}},"outputId":"31a06feb-5869-4cd5-8d03-ab50e7e551be","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import pandas as pd\n","test_df = pd.read_csv('/content/drive/My Drive/tweet-sentiment-extraction/test.csv')\n","test_df.shape"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3534, 3)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"qL0AeJmwlh_5","executionInfo":{"status":"ok","timestamp":1603683807017,"user_tz":-330,"elapsed":23685,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}},"outputId":"0876053b-341a-4fb9-95f2-5fe992c88757","colab":{"base_uri":"https://localhost:8080/","height":712}},"source":["import time\n","from prettytable import PrettyTable \n","start = time.time()\n","ip = test_df.sample(10)\n","op = get_predictions(ip)\n","print('*'*50)\n","myTable = PrettyTable([\"Sentiment\", \"Input text\", \"Output text\"]) \n","for i in range(len(op)):\n","  myTable.add_row([ip['sentiment'].values[i],ip['text'].values[i],op[i]])\n","\n","print(myTable)  \n","print('Time taken:',time.time()-start)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["All model checkpoint layers were used when initializing TFRobertaForQuestionAnswering.\n","\n","All the layers of TFRobertaForQuestionAnswering were initialized from the model checkpoint at /content/drive/My Drive/tweet-sentiment-extraction/mymodel_pretrained.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["**************************************************\n","Loaded Pretrained TFRobertaForQuestionAnswering model\n","**************************************************\n"],"name":"stdout"},{"output_type":"stream","text":["10it [00:00, 680.77it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loaded trained model\n","**************************************************\n","Preprocessing input data\n","**************************************************\n","Loading Pretrained Tokenizer for TfRoberta model\n","**************************************************\n","Getting input_ids and attention_mask for the input\n","**************************************************\n","Shape of input id and attention mask: (10, 128) (10, 128)\n","**************************************************\n","Predicting o/p value\n","**************************************************\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Successfully ran!!!!!\n","**************************************************\n","**************************************************\n","+-----------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n","| Sentiment |                                                              Input text                                                             |                             Output text                              |\n","+-----------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n","|  negative |                        wow, its hot and miserable. people are probably killing themselves right about now...                        |                      wow, its hot and miserable.                     |\n","|  negative |                                                have to go to the dreaded dmv tomorrow                                               |                                dreaded                               |\n","|  negative |        if recent experience is anything to go by, i fear i might be going off indian food. this is not good, people. not good       |                  this is not good, people. not good                  |\n","|  negative |       sorry demi, just read post that you can attend bgt. im sure your support for sb has helped tho  a world without paps plzz     |                                 sorry                                |\n","|  neutral  |                                  tgif is right...i think i broke my toe last night - on my bad foot.                                |  tgif is right...i think i broke my toe last night - on my bad foot. |\n","|  positive |     knackered! been awake since 5 as couldn`t sleep! just started work now. coffee and toast sounds awesome  chris has been busy!   |                                awesome                               |\n","|  negative |                    kinda doing nothing. life is boring. i feel like changing my look. let`s go shopping tomorrow                    |                                boring.                               |\n","|  neutral  |                                                              follow me??                                                            |                              follow me??                             |\n","|  neutral  |                                                        i don`t saw the movie                                                        |                         i don`t saw the movie                        |\n","|  negative | i`m off to bed. head just won`t stop giving me pain. ahgg.. let this sinus, allergy, whatever you are be over tomorrow! sogni d`oro |                                 pain.                                |\n","+-----------+-------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------+\n","Time taken: 18.24111008644104\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qZT712pJ6Unl"},"source":["**Function 2**\n","\n","  - Take raw data as input and return performance metric\n","\n","  "]},{"cell_type":"code","metadata":{"id":"STbgeTi-lh8l","executionInfo":{"status":"ok","timestamp":1603683807018,"user_tz":-330,"elapsed":23679,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":["def jaccard(str1, str2): \n","    a = set(str(str1).lower().split()) \n","    b = set(str(str2).lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))\n","\n","def get_performance_metric(input):\n","  \"\"\"This function returns the performace metric(Jaccard score) for the given input\"\"\"\n","  from transformers import TFRobertaForQuestionAnswering, TFRobertaModel\n","  roberta = TFRobertaForQuestionAnswering.from_pretrained('/content/drive/My Drive/tweet-sentiment-extraction/mymodel_pretrained')\n","  print('*'*50)\n","  print('Loaded Pretrained TFRobertaForQuestionAnswering model')\n","  print('*'*50)\n","  MAX_LEN=128\n","  import tensorflow as tf\n","  from tensorflow.keras.models import Model\n","  from tensorflow.keras.layers import Input,Softmax,Dense,Activation,Dropout,Flatten\n","\n","\n","  input1 = Input(shape=(MAX_LEN,),name='input_id',dtype=tf.int32)\n","  input2 = Input(shape=(MAX_LEN,),name='attention_mask',dtype=tf.int32)\n","  start_scores,end_scores = roberta(input1,attention_mask = input2)\n","  drop1 = Dropout(0.1)(start_scores)\n","  drop1  = tf.expand_dims(drop1,axis=-1)\n","  layer1 = tf.keras.layers.Conv1D(1,1)(drop1)\n","  layer1= Flatten()(layer1)\n","  softmax1 = Activation('softmax')(layer1)\n","  \n","  drop2 = Dropout(0.1)(end_scores)\n","  drop2  = tf.expand_dims(drop2,axis=-1)\n","  layer2 = tf.keras.layers.Conv1D(1,1)(drop2)\n","  layer2 = Flatten()(layer2)\n","  softmax2 = Activation('softmax')(layer2)\n","\n","  model = Model(inputs=[input1,input2],outputs=[softmax1,softmax2])\n","  model.load_weights('/content/drive/My Drive/tweet-sentiment-extraction/checkpt1/model_roberta.hdf5')\n","  print('Loaded trained model')\n","  print('*'*50)\n","\n","  print('Preprocessing input data')\n","  print('*'*50)\n","  input['text'] = input['text'].apply(lambda x : str(x).lower())\n","  input['selected_text'] = input['selected_text'].apply(lambda x : str(x).lower())\n","\n","  count = input.shape[0]\n","  input_ids = np.zeros((count,MAX_LEN),dtype='int32')\n","  attention_mask = np.zeros((count,MAX_LEN),dtype='int32')\n","\n","  ip_data = input[['text','sentiment']]\n","  print('Loading Pretrained Tokenizer for TfRoberta model')\n","  print('*'*50)\n","  from transformers import RobertaTokenizer\n","  tokenizer = RobertaTokenizer.from_pretrained('/content/drive/My Drive/tweet-sentiment-extraction/roberta_tokenizer',add_prefix_space=True)\n","\n","  print('Getting input_ids and attention_mask for the input')\n","  print('*'*50)\n","  from tqdm import tqdm\n","  for i,each in tqdm(enumerate(ip_data.values)):\n","\n","    val = tokenizer.encode_plus(each[0],each[1],add_special_tokens=True,max_length=128,return_attention_mask=True,pad_to_max_length=True,return_tensors='tf',verbose=False)\n","    input_ids[i] = val['input_ids']\n","    attention_mask[i] = val['attention_mask']\n","\n","  print('Shape of input id and attention mask:',input_ids.shape,attention_mask.shape)\n","  print('*'*50)\n","  print('Predicting o/p value')\n","  print('*'*50)\n","  input_data = (input_ids,attention_mask)\n","  start_pred , end_pred = model.predict((input_data))\n","\n","\n","  pred_text=[]\n","  for i in range(input_ids.shape[0]):\n","    a = np.argmax(start_pred[i])\n","    b=np.argmax(end_pred[i])\n","    text1 = \" \"+\" \".join(ip_data['text'].values[i].split())\n","    enc = tokenizer.encode(text1)\n","    st = tokenizer.decode(enc[a:b+1])\n","    pred_text.append(st)\n","\n","\n","\n","  print('Calcuating the performance metric')\n","  print('*'*50)\n","  actual_text = input['selected_text'].values\n","  scores = []\n","  for i in range(len(pred_text)):\n","    scores.append(jaccard(pred_text[i],actual_text[i]))\n","\n","  res = np.array(scores).mean()\n","  print('Successfully ran!!!!!')\n","  print('*'*50)\n","  return res  "],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4mHswazlh6p","executionInfo":{"status":"ok","timestamp":1603683825431,"user_tz":-330,"elapsed":42085,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}},"outputId":"4cd88126-d3b9-4c1a-ec2d-f8d22d5e7226","colab":{"base_uri":"https://localhost:8080/","height":588}},"source":["import time\n","\n","start = time.time()\n","train_df = pd.read_csv('/content/drive/My Drive/tweet-sentiment-extraction/train.csv')\n","print(train_df.shape)\n","ip = train_df.sample(20)\n","op = get_performance_metric(ip)\n","print('*'*50)\n","print('*'*50)\n","print(\"JACCARD SCORE FOR GIVEN DATA:\",op)\n","print('*'*50)\n","print('*'*50)\n","print('Time taken:',time.time()-start)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["(27481, 4)\n"],"name":"stdout"},{"output_type":"stream","text":["All model checkpoint layers were used when initializing TFRobertaForQuestionAnswering.\n","\n","All the layers of TFRobertaForQuestionAnswering were initialized from the model checkpoint at /content/drive/My Drive/tweet-sentiment-extraction/mymodel_pretrained.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForQuestionAnswering for predictions without further training.\n"],"name":"stderr"},{"output_type":"stream","text":["**************************************************\n","Loaded Pretrained TFRobertaForQuestionAnswering model\n","**************************************************\n"],"name":"stdout"},{"output_type":"stream","text":["20it [00:00, 1050.23it/s]"],"name":"stderr"},{"output_type":"stream","text":["Loaded trained model\n","**************************************************\n","Preprocessing input data\n","**************************************************\n","Loading Pretrained Tokenizer for TfRoberta model\n","**************************************************\n","Getting input_ids and attention_mask for the input\n","**************************************************\n","Shape of input id and attention mask: (20, 128) (20, 128)\n","**************************************************\n","Predicting o/p value\n","**************************************************\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"},{"output_type":"stream","text":["Calcuating the performance metric\n","**************************************************\n","Successfully ran!!!!!\n","**************************************************\n","**************************************************\n","**************************************************\n","JACCARD SCORE FOR GIVEN DATA: 0.7272556446821153\n","**************************************************\n","**************************************************\n","Time taken: 18.24516248703003\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pqe3OZ6ulh3X","executionInfo":{"status":"ok","timestamp":1603683825432,"user_tz":-330,"elapsed":42080,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"3WEvxByxlh0-","executionInfo":{"status":"ok","timestamp":1603683825433,"user_tz":-330,"elapsed":42076,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"YslWw0lylhzR","executionInfo":{"status":"ok","timestamp":1603683825433,"user_tz":-330,"elapsed":42071,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"tBBU_q_QlhwA","executionInfo":{"status":"ok","timestamp":1603683825434,"user_tz":-330,"elapsed":42067,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ojrc1I4zlht9","executionInfo":{"status":"ok","timestamp":1603683825435,"user_tz":-330,"elapsed":42061,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"R_XRbuqhlhq8","executionInfo":{"status":"ok","timestamp":1603683825435,"user_tz":-330,"elapsed":42056,"user":{"displayName":"shriram s","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhAeBwuC62nwyCAhaOF2PMHRtksq28nh5jgvFV_sg=s64","userId":"11573751642360425634"}}},"source":[""],"execution_count":8,"outputs":[]}]}